---
title: "BERT Model Family"
draft: true
date: "2024-12-21"
description: "Overview of BERT family of models."
tags: ["bert", "transformers", "language-modeling"]
---

After publication of Transformers in 2017., the 

Last addition to the BERT family of models is the recently introduced ModernBERT[^6].

In this blog post we'll go through an evolution of BERT-family of models, ... TODO-FINISH

## BERT: Bidirectional Encoder Representations from Transformers

The first language model that put NLP on the world map was BERT[^2].

## RoBERTa: Robust BERT

TODO

## DistilBERT: A distilled version of BERT

TODO

## ALBERT: A Lite BERT

TODO

## DeBERTa

TODO

## ModernBERT: Modern Bidirectional Encoder

TODO

## Honorable mentions

- SciBERT, SentenceBERT, SpanBERT, BERTScore, etc.

[^1]: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
[^2]: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
[^3]: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
[^5]: [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
[^4]: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
[^5]: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)
[^6]: [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/abs/2412.13663)